# -*- coding: utf-8 -*-
"""PyOffice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MeWwYxGbOyjlNpG7j3jzWzqeIO5VXA3p

<div align="center">

# **PYTHON OFFICE**

<div align="Left">

#### **Author:** AMoazeni
#### **Licence:** MIT 

<div align="left">

Business workflow automation tools with Python. Free up time by automating routine and repetitive office taks with batch management of Spreadsheets, Wordprocessor Documents, and Emails.

<div align="center">

<br>

<img sr='https://media.giphy.com/media/fsoCk5kgOcYMM/giphy.gif' width='70%'>

<img sr='https://media.giphy.com/media/l1IY5NRhxdCJYxsmA/giphy.gif' width='70%'>


<br><br>

<div align="Left">

# **0. RUN THE PY-OFFICE NOTEBOOK**

### 0.1 Run In The Cloud (Google Colab Notebook)
Python is already installed on a Google Colab Notebook cloud instances. You need a Gmail account to use Colab Notebooks, and Google provides 12GB RAM and 50GB Storage on their cloud.

1. Go to the [Py-Office Colab Notebook](https://colab.research.google.com/drive/1MeWwYxGbOyjlNpG7j3jzWzqeIO5VXA3p)

2. Click the 'Open In Playground' button on the top left of the screen.

3. Log-in using a Gmail account.

4. Click the the '▶️ Run Cell' buttons on the Left side of each code cell or run all cells with **Runtime --> Run All**.

<br>

### 0.2 Local Installation (Python Anaconda - Jupyter Notebook)
Use this link to [install Python Anaconda 3.X](https://www.anaconda.com/distribution/) on your local computer and use Jupyter Notebook to open the [Py-Office Notebook](https://github.com/AMoazeni/Py-Office/archive/master.zip) found at 'Github.com/AMoazeni/Py-Office'.

```python
git clone https://github.com/AMoazeni/Py-Office.git
cd Py-Office
pip install -r requirements.txt
cd Py-Office/Notebooks
jupyter notebook Py-Office.ipynb 
```

<br>

### 0.3 Install Dependencies
Click the '▶️ Run Cell' button next to the code cell below. You can also select the code cell and run the code with the '**CRTL + ENTER**' keyboard shortcut.

The '!pip' magic command lets you install open-source Python libraries. Anaconda pre-install several popular Python libraries like TensorFlow and NumPy. Get a full list of the [1,500+ Python Pachages Installed with Anaconda](https://docs.anaconda.com/anaconda/packages/pkg-docs/).

```
# Py-Office Requirements.txt
bs4 == 0.0.1
pandas == 0.25.3
selenium == 3.141.0
ipywidgets == 7.5.1
numpy == 1.17.3
matplotlib = 3.1.1

```
"""

!pip list

!pip install bs4 selenium pandas ipywidgets

!jupyter nbextension enable --py widgetsnbextension







"""# **1 . IMPORT .CSV DATASET**

Install [Python 3.X Anaconda](https://www.anaconda.com/distribution/#download-section) and download the [PyOffice Github](https://www.github.com/amoazeni/PyOffice) Repository. Unzip the folder and place your .CSV Dataset in the '**CSV Data**' folder.

<div align="center" >
<img src='https://github.com/AMoazeni/PyOffice/blob/master/img/git.png?raw=true' width=70%>
</div>
<br>

### 1.1 Export .CSV Data From Microsoft Excel.
<div align="center" >
<img src='https://cdn.ablebits.com/_img-blog/excel-csv/save-excel-csv.png' width=70%>
</div>
<br>

### 1.2 Export .CSV Data From Google Docs.
<div align="center" >
<img src='https://lh3.googleusercontent.com/wtKtoKgudmVkaI1pRc2SeN25GsATlFB54HgVv57rf9akOA4H_A6TLT3g67yC_VUnRq2E-WetMIAdbNp_7NuISPRsqXAi7S7NIGRzrD6c34nz1EGFqifH84R09p0XxrfBNIl5mSnl' width=70%>
</div>
<br>
"""



"""# **2. SPREADSHEETS (XLSX, CSV)**
Comma-Separated Values (CSV) is a tabular file format where each line of the file is a data record. CSV files use commas to separate values into rows and columns.

Example CSVs found on [Kaggle.com](https://www.kaggle.com/datasets?sort=votes&fileType=csv).
"""

import os
import csv
import re
from urllib.request import urlopen
from bs4 import BeautifulSoup

# Get HTML file from URL as a Beautiful Soup object
html = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')
bs = BeautifulSoup(html, 'html.parser')

# Create a CSV file for each table in the page
tables = bs.find('table',{'class':'wikitable'})[0]
rows = tables.find('tr')
for table in range(len(tables)):
  print('Table {}:\n'.format(table), tables[table].find('td'), end='\n\n\n\n\n\n\n\n')

#csvFile = open('editors.csv', 'wt+')
#writer = csv.writer(csvFile)

# Get URL Links from Page
html = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')
bs = BeautifulSoup(html, 'html.parser')
tables = bs.find_all('a')
for tag in bs.find_all('a', href = re.compile('http')):
  if 'href' in tag.attrs:
    print(tag.attrs['href'])

import os
import csv
import re
from urllib.request import urlopen
from bs4 import BeautifulSoup
from IPython.display import Image
from IPython.core.display import HTML 

html = urlopen('https://en.wikipedia.org/wiki/2019_Rugby_World_Cup')
bs = BeautifulSoup(html, 'html.parser')
tables = bs.find_all('a')
for tag in bs.find_all('img'):
  # Print Image 
  print(tag.attrs['src'])

for i in range(len(bs.find_all('img'))):
  print('Image {}:\n'.format(i))
  Image(url = bs.find_all('img')[i].attrs['src'])

# //upload.wikimedia.org/wikipedia/en/thumb/b/be/Flag_of_England.svg/23px-Flag_of_England.svg.png

import os
import csv
import re
from urllib.request import urlopen
from bs4 import BeautifulSoup
from IPython.display import display, Image
from IPython.core.display import HTML
import requests


session = requests.Session()
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36'}

root_url = 'https://www.yahoo.com'
req = session.get(root_url, headers=headers)
bs = BeautifulSoup(req.text, 'html.parser')

# Standard Python URLlib
#html = urlopen(root_url)
#bs = BeautifulSoup(html, 'html.parser')

img_num = len(bs.find_all('img'))
count = img_num

for image in bs.find_all('img'):
  print('\n\nImage {}:'.format(img_num - count))
  count = count - 1
  try:
    img_url = image.attrs['src']
    print(img_url)
    display(Image(url=img_url, retina=True))
  except:
    img_url = image.contents
    print(img_url)

#!/usr/bin/env python
from re import findall,sub
from lxml import html
from time import sleep
from selenium import webdriver
from pprint import pprint
from xvfbwrapper import Xvfb


def parse(url):
    searchKey = "Las Vegas" # Change this to your city 
    checkInDate = '27/08/2016' #Format %d/%m/%Y
    checkOutDate = '29/08/2016' #Format %d/%m/%Y
    response = webdriver.Firefox()
    response.get(url)
    searchKeyElement = response.find_elements_by_xpath('//input[contains(@id,"destination")]')
    checkInElement = response.find_elements_by_xpath('//input[contains(@class,"check-in")]')
    checkOutElement = response.find_elements_by_xpath('//input[contains(@class,"check-out")]')
    submitButton = response.find_elements_by_xpath('//button[@type="submit"]')
    if searchKeyElement and checkInElement and checkOutElement:
        searchKeyElement[0].send_keys(searchKey)
        checkInElement[0].clear()
        checkInElement[0].send_keys(checkInDate)
        checkOutElement[0].clear()
        checkOutElement[0].send_keys(checkOutDate)
        randomClick = response.find_elements_by_xpath('//h1')
        if randomClick:
            randomClick[0].click()
        submitButton[0].click()
        sleep(15)
        dropDownButton = response.find_elements_by_xpath('//fieldset[contains(@id,"dropdown")]')
        if dropDownButton:
            dropDownButton[0].click()
            priceLowtoHigh = response.find_elements_by_xpath('//li[contains(text(),"low to high")]')
            if priceLowtoHigh:
                priceLowtoHigh[0].click()
                sleep(10)

    parser = html.fromstring(response.page_source,response.current_url)
    hotels = parser.xpath('//div[@class="hotel-wrap"]')
    for hotel in hotels[:5]: #Replace 5 with 1 to just get the cheapest hotel
        hotelName = hotel.xpath('.//h3/a')
        hotelName = hotelName[0].text_content() if hotelName else None
        price = hotel.xpath('.//div[@class="price"]/a//ins')
        price = price[0].text_content().replace(",","").strip() if price else None
        if price==None:
            price = hotel.xpath('.//div[@class="price"]/a')
            price = price[0].text_content().replace(",","").strip() if price else None
        price = findall('([\d\.]+)',price) if price else None
        price = price[0] if price else None
        rating = hotel.xpath('.//div[@class="star-rating"]/span/@data-star-rating')
        rating = rating[0] if rating else None
        address = hotel.xpath('.//span[contains(@class,"locality")]')
        address = "".join([x.text_content() for x in address]) if address else None
        locality = hotel.xpath('.//span[contains(@class,"locality")]')
        locality = locality[0].text_content().replace(",","").strip() if locality else None
        region = hotel.xpath('.//span[contains(@class,"locality")]')
        region = region[0].text_content().replace(",","").strip() if region else None
        postalCode = hotel.xpath('.//span[contains(@class,"postal-code")]')
        postalCode = postalCode[0].text_content().replace(",","").strip() if postalCode else None
        countryName = hotel.xpath('.//span[contains(@class,"country-name")]')
        countryName = countryName[0].text_content().replace(",","").strip() if countryName else None

        item = {
                    "hotelName":hotelName,
                    "price":price,
                    "rating":rating,
                    "address":address,
                    "locality":locality,
                    "region":region,
                    "postalCode":postalCode,
                    "countryName":countryName,
        }
        pprint(item)
if __name__ == '__main__':
#    vdisplay = Xvfb()
#    vdisplay.start()
    driver = webdriver.Firefox(executable_path="/usr/bin/geckodriver")
    !chmod +x geckodriver
    parse('http://www.hotels.com')
#    vdisplay.stop()

import datetime
from bs4 import BeautifulSoup
import time
import selenium
from selenium import webdriver
import re

keyword = 'blue+skateboard'
driver = webdriver.Chrome()

url = 'https://www.amazon.co.uk/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords={}'

driver.get(url.format(keyword))
soup = BeautifulSoup(driver.page_source, 'lxml')
results = soup.select('.s-result-list [data-asin]')

for a, b in enumerate(results):
    soup = b
    header = soup.select_one('h5')
    result = a + 1
    title = header.text.strip()

    try:
        link = soup.select_one('h5 > a')
        url = link['href']
        url = re.sub(r'/ref=.*', '', str(url))
    except:
        url = "None"

    if url !='/gp/slredirect/picassoRedirect.html':
        ASIN = re.sub(r'.*/dp/', '', str(url))
        #print(ASIN)

        try:
            score = soup.select_one('.a-icon-alt')
            score = score.text
            score = score.strip('\n')
            score = re.sub(r' .*', '', str(score))
        except:
            score = "None"

        try:
            reviews = soup.select_one("href*='#customerReviews']")
            reviews = reviews.text.strip()
        except:
            reviews = "None"

        try:
            PRIME = soup.select_one('[aria-label="Amazon Prime"]')
            PRIME = PRIME['aria-label']
        except:
            PRIME = "None"
        data = {keyword:[keyword,str(result),title,ASIN,score,reviews,PRIME,datetime.datetime.today().strftime("%B %d, %Y")]}
        print(data)

for row in rows:
  csvRow = []
for cell in row.findAll(['table']):
  csvRow.append(cell.get_text())
  writer.writerow(csvRow)
  print(cell)

csvFile.close()
# Print CSV File
os.chdir(os.getcwd())
with open('editors.csv', newline='') as csvfile:
  spamreader = csv.reader(csvfile, delimiter=',')
  for row in spamreader:
    print(', '.join(row))

"""### 2.1 Get Data From Web"""

import csv
import os
import pandas as pd

def csv_import(csv_online_path = 'https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Data/Google_Stock_Price_Train.csv'):
  try:
    
    pd_csv = pd.read_csv(csv_online_path)
    
    with open('new.csv', 'w+', newline='') as f:
      writer = csv.writer(f)
      writer.writerows(pd_csv)
      print('CSV_Import function completed for CSV for path:\n', csv_online_path, end='\n\n')
      return pd_csv

  except BaseException as e:
    print('Error: ', e, end='\n\n')
    return e

  finally:
    print('End Of CSV Import Function.', end='\n\n')

csv_online_path = 'https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Data/Google_Stock_Price_Train.csv'
csv_import(csv_online_path)

"""### 2.2 Import from MS Excel and Google Sheets"""



import pandas as pd

csv_online_path = 'https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Stock-Market-Prediction/master/Data/Google_Stock_Price_Train.csv'
pd_csv = pd.read_csv(csv_online_path)
#pd_csv.info()
#rows = pd_csv.shape[0]
#print('Rows: ', row_len)
#print('Rows: ', col_len)
#pd_csv.describe()


csv_file = 'new.csv'

root_path = os.getcwd()
csv_path = root_path + '/csv_data'


try:
  if not os.path.exists(csv_path):
    os.mkdir(csv_path)
    os.chdir(csv_path)
    print('New csv_data folder created at:\n', csv_path, end='\n\n')

  else:
    os.chdir(csv_path)
    print('csv_data folder already existed, current working directory set to:\n', csv_path, end='\n\n')
      
  with open(csv_file, 'w+', newline='') as f:
    pd_csv.to_csv(f, sep='\t', encoding='utf-8')
    print('CSV file "{}" written to path:\n'.format(csv_file), csv_path, end='\n\n')
    f.close()
    
except BaseException as e:
  print('Error Creating CSV_Data Folder: ', e, end='\n\n')

finally:
  os.chdir(root_path)
  print('Finally, the working directory is set back to:\n', root_path, end='\n\n')

print(root_path)
print(csv_path)
print(csv_file)
os.chdir('/content')
print(os.getcwd())

# Commented out IPython magic to ensure Python compatibility.
import os
print(os.chdir('/content'))
print('Current Working Directory: \n', os.getcwd(), end='\n\n')
os.ch
# %ls



"""## 2.3 Data Visualization

# **3. WORD PROCESSOR (DOCX, PDF, TXT)**
"""

# Install these libraries: 'pip3 install python-docx', 'pip install pypandoc'

from docx import Document
import csv
import os
import shutil
import pypandoc
import subprocess

subprocess.run(["ls", "-l"])

# Set this folder as 'working directory'.

# Path to MS Word .DOCX document template and MS Excel database.
# Add custom keywords to first of .CSV file and .DOCX template.
DOC_NAME = 'Document.docx'
SHEET_NAME = 'Sheet.csv'

RES_DIR = 'Results'
RES_FILE_NAME = 'Result.docx'

# Create results folder.
if os.path.exists(RES_DIR):
    shutil.rmtree(RES_DIR)
os.makedirs(RES_DIR)


# Function: Replace keywords in Document with data from .CSV database.
# Save Document as .PDF in folders created in the root directory.
def CSV_To_PDF(keywords, info, count):    
    doc = Document(DOC_NAME)
    
    # Replace loop for each column
    for i in range(len(keywords)):
        for p in doc.paragraphs:
            if keywords[i] in p.text:
                inline = p.runs
                # Loop added to work with runs (strings with same style)
                for j in range(len(inline)):
                    if keywords[i] in inline[j].text:
                        text = inline[j].text.replace(keywords[i], info[i])
                        inline[j].text = text

    file_path = RES_DIR + '/' + str(count) + ' - ' + str(info[1]) + ', ' + str(info[2]) + '/'
    
    # Create results folder.
    if os.path.exists(file_path):
        shutil.rmtree(file_path)
    os.makedirs(file_path)
        
    file_path_save = file_path + RES_FILE_NAME
    doc.save(file_path_save)
    
    return 1


# Create a blank document.
#doc = Document()
 
# Add a styled paragraph.
#p = doc.add_paragraph('Lorem ipsum dolor sit amet.', style='Heading 1')
#p.add_run('bold').bold = True
#p.add_run(' and some ')
#p.add_run('italic.').italic = True

# Add a simple paragraph.
#doc.add_paragraph('Lorem ipsum dolor sit amet.', style='paragraph')

# Open an existing document.
doc = Document(DOC_NAME)

# Parse CSV    
csv_read = csv.reader(open(SHEET_NAME, 'rt'))
rows = []

for line in csv_read:
  print(line)
  rows.append(line)

count = 0
       
for row in rows:
    keywords = rows[0]
    info = row
    CSV_To_PDF(keywords, info, count)
    count += 1

"""# **4. EMAIL (GMAIL, OUTLOOK)**"""



"""# PyOffice Object
Document Processing, SpreadSheets, and Email with Python.
"""

import os
import csv
import re
from urllib.request import urlopen
from bs4 import BeautifulSoup

# Sample Data
my_url = 'https://www.yahoo.com/'

my_urls = ['https://www.yahoo.com/',
            'https://www.microsoft.com/',
            'https://www.google.com/'
          ]

# Get HTML file from URL as a Beautiful Soup object
html = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')
bs = BeautifulSoup(html, 'html.parser')
bs.find('title')

urls = []
soups = []

for arg_index, url in enumerate(my_urls):
    print('Index = [{}]:\n'.format(arg_index), url, end='\n\n')
    urls.append(url)
    bs = BeautifulSoup(urlopen(url), 'html.parser')
    soups.append(bs)
    print(bs.find('title'))

soup = BeautifulSoup(urlopen(my_url), 'html.parser')
print('URL: ', url, end='\n')
print('URL Title: ', bs.find('title'), end='\n')

print('URLs:\n', urls)

#!/usr/bin/env python

# Import Libraries
import csv
import re
from urllib.request import urlopen

import bs4 as BeautifulSoup
import pandas


#import ipywidgets
#import selenium
#import numpy
#import matplotlib

# Print Imported Library Methods
#lib_methods = dir(csv)
#print('CSV Built-In Library Methods:\n', lib_methods, end='\n\n')

# Sample Class Definition
class PyOffice:
  def __init__(self):
    self.sample_website = 'https://www.yahoo.com/'
    self.sample_csv = 'https://raw.githubusercontent.com/AMoazeni/PyOffice/master/googleplaystore.csv'
    pass

  def print_name(self):
    print("The class name is " + self.name)

  def OpenUrl(self, *args, **kwargs):
    # Initialize
    print('', end='\n\n')
  
    # Try block
    try:
      # Iterating over Python *args itterators
      print('Opening Url:\n')
      soup = BeautifulSoup(urlopen(args), 'html.parser')
      print('URL: ', url, end='\n')
      print('URL Title: ', bs.find('title'), end='\n')

      print('URLs:\n', urls)
  
      return soup
  
    # Error Handler
    except BaseException as e:
      print('Error: \n', e, end='\n\n')
      return e
  
    # Clean up block
    finally:
      pass



# Sample Data
my_url = 'https://www.yahoo.com/'

my_urls = ['https://www.yahoo.com/',
            'https://www.microsoft.com/',
            'https://www.google.com/'
          ]
  
# OpenUrls Method
office = PyOffice()

office.OpenUrl()
  
# Print <title> tag of soups returned by OpenUrls
#for soup in soups:
#  print(soup.find('title'))

#print(soup)

class Person:
  def __init__(self, name, age):
    self.name = name
    self.age = age

  def myfunc(self):
    print("Hello my name is " + self.name)

p1 = Person("John", 36)
p1.myfunc()

Person

from urllib.request import urlopen
from bs4 import BeautifulSoup
import csv


html = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')

lib_methods = dir(html)
print('HTML Built-In Library Methods:\n', lib_methods, end='\n\n')

soup = BeautifulSoup(html, 'html.parser')
#print(type(soup.prettify()))

with open('site.html', 'w+', newline='') as f:
  html = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')
  mybytes = html.read()
  mystr = mybytes.decode("utf8")
  html.close()
  f.write(mystr)
f.close()

#print(type(lines))
#print(len(x))

#for line in lines:
#print(lines)

"""# Widgets"""

# Source article
# https://towardsdatascience.com/bring-your-jupyter-notebook-to-life-with-interactive-widgets-bc12e03f0916



import ipywidgets as widgets
from IPython.display import display
import pandas as pd
import numpy as np

# All available widgets from the PyWidgets library
print('All available ipywidgets:\n', dir(widgets), end='\n\n\n')


# Bounded Float Widget
print('Bounded Float Widget:\n')
bounded_num = widgets.BoundedFloatText(min=0, max=100000, value=8, step=1, description='Bounded Float Text:')
display(bounded_num)
print('', end='\n\n\n')


# Slider Widget
print('Slider Widget:\n')
slider = widgets.IntSlider(min=0, max=10, value=3, step=1, description='Slider:')
display(slider)
#print('Slider Starting Value: \n', slider.value, end='\n\n')
print('', end='\n\n\n')


# Linked Slider Widget
print('Linked Slider Widget:\n')
slider_joined = widgets.IntSlider(min=0, max=10, value=5, step=1, description='Linked Slider:')
text_joined = widgets.IntText(description='Linked Text:')
display(slider_joined, text_joined)
widgets.jslink((slider_joined, 'value'), (text_joined, 'value'));
print('', end='\n\n\n')


# Button Widget
print('Button Widge:\n')
btn = widgets.Button(description='Sample Button')
display(btn)
def btn_eventhandler(obj):
    print('{} pressed!'.format(obj.description))
btn.on_click(btn_eventhandler)
print('', end='\n\n\n')


# Import Pandas DataFrame From Online CSV
url = "https://data.london.gov.uk/download/number-international-visitors-london/b1e0f953-4c8a-4b45-95f5-e0d143d5641e/international-visitors-london-raw.csv"
df_london = pd.read_csv(url)


# Load Data as Widget
ALL = 'ALL'
def unique_sorted_values_plus_ALL(array):
    unique = array.unique().tolist()
    unique.sort()
    unique.insert(0, ALL)
    return unique

dropdown_year = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_london.year))

def dropdown_year_eventhandler(change):
    if (change.new == ALL):
        display(df_london)
    else:
        display(df_london[df_london.year == change.new])

dropdown_year.observe(dropdown_year_eventhandler, names='value')
display(dropdown_year)

# Capture Output
output_year = widgets.Output()

def dropdown_year_eventhandler(change):
    output_year.clear_output()
    with output_year:
        display(df_london[df_london.year == change.new])

display(output_year)

output = widgets.Output()

dropdown_year = widgets.Dropdown(options =    unique_sorted_values_plus_ALL(df_london.year))
dropdown_purpose = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_london.purpose))

def common_filtering(year, purpose):
    output.clear_output()
    
    if (year == ALL) & (purpose == ALL):
        common_filter = df_london
    elif (year == ALL):
        common_filter = df_london[df_london.purpose == purpose]
    elif (purpose == ALL):
        common_filter = df_london[df_london.year == year]
    else:
        common_filter = df_london[(df_london.year == year) & 
                                  (df_london.purpose == purpose)]
    
    with output:
        display(common_filter)


def dropdown_year_eventhandler(change):
    common_filtering(change.new, dropdown_purpose.value)
def dropdown_purpose_eventhandler(change):
    common_filtering(dropdown_year.value, change.new)


dropdown_year.observe(
dropdown_year_eventhandler, names='value')
dropdown_purpose.observe(
dropdown_purpose_eventhandler, names='value')

def colour_ge_value(value, comparison):
    if value >= comparison:
        return 'color: red'
    else:
        return 'color: black'

def common_filtering(year, purpose, num):
  with output:
        display(common_filter
                .style.applymap(
                    lambda x: colour_ge_value(x, num),
                    subset=['visits','spend', 'nights']))
        
def dropdown_year_eventhandler(change):
    common_filtering(change.new, dropdown_purpose.value,
                     bounded_num.value)
def dropdown_purpose_eventhandler(change):
    common_filtering(dropdown_year.value, change.new, 
                     bounded_num.value)
    
def bounded_num_eventhandler(change):
    common_filtering(dropdown_year.value, dropdown_purpose.value, 
                     change.new)
bounded_num.observe(bounded_num_eventhandler, names='value')



url = "https://data.london.gov.uk/download/number-international-visitors-london/b1e0f953-4c8a-4b45-95f5-e0d143d5641e/international-visitors-london-raw.csv"
df = pd.read_csv(url)

df.row





"""# Python Sample Function"""

#!/usr/bin/env python

# Import Libraries
import csv

# Print Imported Library Methods
csv_methods = dir(csv)
print('CSV Built-In Library Methods:\n', csv_methods, end='\n\n')

# Sample Class Definition
class MyClass:
  def __init__(self):
    #self.name = name
    #self.value = value
    pass

  def print_name(self):
    print("The class name is " + self.name)

  def my_function(*args, **kwargs):
    # Initialize
    print('', end='\n\n')
  
    # Try block
    try:
      arg_list = []
  
      # Iterating over Python *args itterators
      print('Itterator *args:\n')
      for arg_index, arg in enumerate(args):
          print('Index = [{}], Type = {}:\n'.format(arg_index,type(arg)), arg, end='\n\n')
          arg_list.append(arg)
  
      # Iterating over Python **kwargs dictionaries
      print('\nKey-Value **kwargs:\n')
      for kwarg_index, (key, value) in enumerate(kwargs.items()):
        print('Index [{}]:\n'.format(kwarg_index),
              '\bKey: ', key,
              '\nValue: ', value,
              end='\n\n')
  
      return arg_list
  
    # Error Handler
    except BaseException as e:
      print('Error: \n', e, end='\n\n')
  
      return e
  
    # Clean up block
    finally:
      pass




# Main Loop
if __name__== "__main__":

    # Sample Data
    my_list = [[1, 2, 3],
               (4,5,6),
               {'key1': 'value1','key2': 'value2','key3': 3},
               'a'
               ]
    
    # Sample Class
    c1 = MyClass()
    print(c1)
    
    # Call function for every list item
    print('Multiple Function Calls With Star "*" Itterator:')
    c1.my_function(*my_list, a=3, b='sample_text');
    
    # Send entire list to function at once
    print('\n\n\n\nSingle Argument Function Calls:')
    c1.my_function(my_list, a=3, b='sample_text');

x = MyClass()
x.my_function()





"""# PyMongo Database Storage"""

import pymongo

myclient = pymongo.MongoClient("mongodb://localhost:27017/")
mydb = myclient["mydatabase"]
mycol = mydb["customers"]

mylist = [
  { "name": "Amy", "address": "Apple st 652"},
  { "name": "Hannah", "address": "Mountain 21"},
  { "name": "Michael", "address": "Valley 345"},
  { "name": "Sandy", "address": "Ocean blvd 2"},
  { "name": "Betty", "address": "Green Grass 1"},
  { "name": "Richard", "address": "Sky st 331"},
  { "name": "Susan", "address": "One way 98"},
  { "name": "Vicky", "address": "Yellow Garden 2"},
  { "name": "Ben", "address": "Park Lane 38"},
  { "name": "William", "address": "Central st 954"},
  { "name": "Chuck", "address": "Main Road 989"},
  { "name": "Viola", "address": "Sideway 1633"}
]

x = mycol.insert_many(mylist)

#print list of the _id values of the inserted documents:
print(x.inserted_ids)





"""# PyOffice Functions"""

import csv

def csv_open(*args, **kwargs):
  ''' csv_open(str 'csv_path') returns a Python Dictionary{} and accepts a local .CSV file path.
      
      List of columns: "csv_data['columns'][0]"

      List of rows: "csv_data['rows']"

      First row: "csv_data['rows'][0]"

      First item of first row: "csv_data['rows'][0][0]"
  '''
  
  # Initialize
  print('', end='\n\n')
  csv_data={}
  rows=[]
  columns=[]
  index=0

  # Function Block
  try:
    with open(args[0], newline='') as csvfile:
      csv_reader = csv.reader(csvfile)
      
      for row in csv_reader:
        if index==0:
          columns.append(row)
        else:
          rows.append(row)
        index+=1
        #print(', '.join(row))

      csv_data['columns'] = columns
      csv_data['rows'] = rows
    
    return csv_data

  # Error Handler
  except BaseException as e:
    error_msg = 'Error: \n' + str(e)
    print(error_msg, end='\n\n')
    return error_msg

  # Clean up block
  finally:
    pass

#csv_url = '/content/sample_data/mnist_train_small.csv'
csv_url = '/content/sample_data/california_housing_train.csv'
csv_data = csv_open(csv_url)

print('Response Length:\n', len(csv_data), end='\n\n')
print('Response Type:\n', type(csv_data), end='\n\n')

if str(type(csv_data)) == "<class 'dict'>":
  columns = csv_data['columns'][0]
  rows = csv_data['rows']
  
  print('Columns (', len(columns), '):\n', columns, end='\n\n')
  print('Rows (', len(rows), '):')
  for i in range(5):
    print(rows[i], end='\n')

# Column Aggregate and Average

columns = csv_data['columns'][0]
rows = csv_data['rows']

for column in range(len(columns)):
  total_sum=0
  for row in rows:
    try:  
      total_sum = total_sum + float(row[column])
    except:
      total_sum = total_sum + 0

  print('\n\nColumn: ', columns[column],
        '\nTotal Sum: ', total_sum,
        '\nAverage: ', total_sum/len(rows),
        end='\n\n')











import csv

def csv_open(*args, **kwargs):
  ''' csv-open'''
  
  # Initialize
  print('', end='\n\n')



  # Function Block
  try:
    
    return args

  # Error Handler
  except BaseException as e:
    error_msg = 'Error: \n' + e
    print(error_msg, end='\n\n')
    return error_msg

  # Clean up block
  finally:
    pass

# Web Application

from urllib.request import urlopen
import requests
import os

url = 'http://www.google.com'

html = urlopen(url)

import socket; s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM);s.connect(("8.8.8.8", 80));print(s.getsockname()[0])







# Python Yield Function - Function continues running

def nextSquare(): 
    i = 1; 
  
    # An Infinite loop to generate squares  
    while True: 
        yield i*i                 
        i += 1  # Next execution resumes  
                # from this point      
  
# Driver code to test above generator  
# function 
for num in nextSquare(): 
    if num > 100: 
         break    
    print(num)



openpagetest(str(type(OpenPage('url'))), '<class dict>')
getcode

# Python Unit Test

import unittest

def add(a,b):
  return a+b

class TestDemo(unittest.TestCase):
    """Example of how to use unittest in Jupyter."""
    
    def test_pass(self):
        self.assertEqual(add(2,2), 4)

    def test_fail(self):
        self.assertEqual(add(2,2), 5)

if __name__ == '__main__':
    unittest.main(argv=[''], exit=False)